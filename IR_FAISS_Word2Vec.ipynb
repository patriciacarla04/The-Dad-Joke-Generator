{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqyLdv5Qn-t5"
      },
      "source": [
        "**The Dad Joke Generator**\n",
        "\n",
        "An information retrieval algorithm that I created as my final project for the Information Extraction course at university.\n",
        "\n",
        "I worked with a dad joke dataset found on Kaggle: https://www.kaggle.com/datasets/oktayozturk010/reddit-dad-jokes & \n",
        "\n",
        "a short jokes dataset, found on Kaggle as well: https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "m-D6ACIDn-ue"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S-YomO6hn-ug"
      },
      "outputs": [],
      "source": [
        "#Loading the datasets\n",
        "\n",
        "df_shortjokes = pd.read_csv('shortjokes.csv')\n",
        "\n",
        "df_dadjokes = pd.read_csv('reddit_dadjokes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JsUqR-ZMn-ug",
        "outputId": "177227c1-cc81-4313-c1a3-eb84c9656fb4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>joke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Doctor: \"So, you're telling me that you have a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A grizzly kept talking to me and annoyed me He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I rubbed mayonnaise on my eyes Oh fuck oh shit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What do you say to encourage an asteroid? Go l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>They always ask me why my mood is always negat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                joke\n",
              "0  Doctor: \"So, you're telling me that you have a...\n",
              "1  A grizzly kept talking to me and annoyed me He...\n",
              "2  I rubbed mayonnaise on my eyes Oh fuck oh shit...\n",
              "3  What do you say to encourage an asteroid? Go l...\n",
              "4  They always ask me why my mood is always negat..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Removing unwanted columns\n",
        "\n",
        "df_dadjokes = df_dadjokes.drop(['url','score','date', 'author'], axis=1)\n",
        "\n",
        "df_dadjokes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1aK1KpluwBT",
        "outputId": "fa704983-4c98-47c8-cf4b-a8a50d91f94a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/patriciagrigor/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/patriciagrigor/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Importing modules for preprocessing pipeline\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUDRx4liu8KB",
        "outputId": "fde9c41e-b274-43df-e993-7d0b2d1097be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/patriciagrigor/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TuDRl73uuVj3"
      },
      "outputs": [],
      "source": [
        "#A class for the preprocessing pipeline which can be reused & adapted for several documents\n",
        "\n",
        "class PreprocessingPipeline:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemming = PorterStemmer()\n",
        "        self.punctuation = string.punctuation\n",
        "    \n",
        "    #Converting text into tokens\n",
        "    def tokenize(self, text):\n",
        "        return word_tokenize(text)\n",
        "    \n",
        "    #Converting the tokens to lowercase\n",
        "    def case_fold(self, token):\n",
        "        return token.lower()\n",
        "    \n",
        "    #Removing stop-words\n",
        "    def remove_stop_words(self, token):\n",
        "        if token is not None and token not in self.stop_words:\n",
        "            return token\n",
        "        return None\n",
        "\n",
        "    #Removing unwanted characters\n",
        "    def remove_unwanted_characters(self, token):\n",
        "        if token is not None and not token.isalpha():\n",
        "            return None\n",
        "        return token\n",
        "    \n",
        "    #Lemmatizing tokens\n",
        "    def lemmatize(self,token):\n",
        "        lemmatized_token = self.lemmatizer.lemmatize(token)\n",
        "        return lemmatized_token\n",
        "   \n",
        "    def token_stemmer(self,token):\n",
        "        stemmed_token = self.stemming.stem(token)\n",
        "        return stemmed_token\n",
        "\n",
        "    #Preprocessing text by applying all steps from above\n",
        "    def preprocess_text(self, text):\n",
        "        tokens = self.tokenize(text)\n",
        "        preprocessed_tokens = []\n",
        "        for token in tokens:\n",
        "            token = self.case_fold(token)\n",
        "            token = self.remove_stop_words(token)\n",
        "            token = self.remove_unwanted_characters(token)\n",
        "            \n",
        "            if token is not None:\n",
        "                token = self.lemmatize(token)\n",
        "                #token = self.token_stemmer(token)\n",
        "                preprocessed_tokens.append(token)\n",
        "        \n",
        "        return preprocessed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I0bzCyR7i3X",
        "outputId": "17aa93b9-2f7e-4351-eaaa-569519b94bb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "447985\n"
          ]
        }
      ],
      "source": [
        "short_jokes = df_shortjokes['Joke'].to_list()\n",
        "dad_jokes = df_dadjokes['joke'].to_list()\n",
        "\n",
        "all_jokes = short_jokes + dad_jokes\n",
        "\n",
        "print(len(all_jokes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etSM9rVouc7W",
        "outputId": "2adbf1a5-2d1a-412e-cdcf-d7e2161c011b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['narrating', 'documentary', 'narrator', 'ca', 'hear', 'saying', 'cuz', 'talking']\n",
            "['telling', 'daughter', 'garlic', 'good', 'good', 'immune', 'system', 'keep', 'pest', 'mosquito', 'vampire', 'men']\n",
            "['going', 'really', 'rough', 'period', 'work', 'week', 'fault', 'swapping', 'tampax', 'sand', 'paper']\n",
            "['could', 'dinner', 'anyone', 'dead', 'alive', 'would', 'choose', 'alive']\n",
            "['two', 'guy', 'walk', 'bar', 'third', 'guy', 'duck']\n"
          ]
        }
      ],
      "source": [
        "preprocessor = PreprocessingPipeline()\n",
        "\n",
        "#Trying out the preprocessing pipeline & comparing it to the original text\n",
        "\n",
        "preprocessed_jokes = [preprocessor.preprocess_text(joke) for joke in all_jokes]\n",
        "\n",
        "for joke in preprocessed_jokes[:5]:\n",
        "    print(joke)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V3mWj3tPwEis"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fz6RJZekwQlu"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(preprocessed_jokes, vector_size=300, window=5, min_count=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save(\"word2vec_jokes_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = Word2Vec.load(\"word2vec_jokes_model.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "oX-GjV-IxTh_"
      },
      "outputs": [],
      "source": [
        "word_embeddings = model.wv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EX52FBvTxoUq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert word embeddings to NumPy array\n",
        "embedding_matrix = np.array([word_embeddings[word] for word in word_embeddings.index_to_key])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9zRAaamRxtQV"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# Initialize FAISS index\n",
        "index = faiss.IndexFlatL2(embedding_matrix.shape[1])  # L2 distance metric\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embedding_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "w101cg1bxx2l"
      },
      "outputs": [],
      "source": [
        "faiss.write_index(index, \"word2vec_index.faiss\")\n",
        "word_embeddings.save(\"word2vec_word_embeddings.kv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_file = 'word2vec_index.faiss'\n",
        "index = faiss.read_index(index_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pTaaZJ0xywrR"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Initialize BM25Okapi with the joke corpus\n",
        "bm25 = BM25Okapi(preprocessed_jokes)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#Normal query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WKe6DH13zBf0"
      },
      "outputs": [],
      "source": [
        "query = \"Corona virus\"  # Example query keyword\n",
        "\n",
        "# Preprocess the query keyword\n",
        "query_tokens = preprocessor.preprocess_text(query)\n",
        "\n",
        "# Get BM25 scores for the jokes based on the query\n",
        "bm25_scores = bm25.get_scores(query_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2Hr-z1i129a6"
      },
      "outputs": [],
      "source": [
        "ranked_jokes_full = sorted(zip(all_jokes, bm25_scores), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "urJmfEERzHyf"
      },
      "outputs": [],
      "source": [
        "# Sort the jokes based on BM25 scores in descending order\n",
        "ranked_jokes = sorted(zip(preprocessed_jokes, bm25_scores), key=lambda x: x[1], reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypeoe1-ozP0M",
        "outputId": "87596bc5-68d2-46d2-848d-98120d475f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(['single', 'person', 'like', 'corona', 'corona', 'corona', 'virus'], 21.66872211038533), (['corona', 'hold', 'virus'], 21.219569152196243), (['corona', 'hold', 'virus'], 21.219569152196243)]\n"
          ]
        }
      ],
      "source": [
        "print(ranked_jokes[0:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCPyMLYe2T_o",
        "outputId": "904c0d0f-d362-4fc1-9775-9ab17df774c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('A single person be like: Corona vir-you + corona vir-me = corona virus.', 21.66872211038533), ('I’ll have a Corona... hold the virus!', 21.219569152196243), ('I’ll have a corona... Hold the virus!', 21.219569152196243)]\n"
          ]
        }
      ],
      "source": [
        "print(ranked_jokes_full[0:3])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Expanded query (with most_similar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "2SLbdNoz4eAY"
      },
      "outputs": [],
      "source": [
        "query = \"virus\"  # Example query keyword\n",
        "\n",
        "# Preprocess the query keyword\n",
        "query_tokens = preprocessor.preprocess_text(query)\n",
        "\n",
        "query_embeddings = [model.wv[token] for token in query_tokens]\n",
        "\n",
        "similar_terms = []\n",
        "for token in query_tokens:\n",
        "    similar_tokens = model.wv.most_similar(token, topn=3)  # Adjust the number of similar terms as desired\n",
        "    similar_terms.extend([sim_term[0] for sim_term in similar_tokens])\n",
        "\n",
        "expanded_query = query_tokens + similar_terms\n",
        "\n",
        "\n",
        "# Get BM25 scores for the jokes based on the query\n",
        "bm25_scores = bm25.get_scores(expanded_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "87ByV73N5Ky8"
      },
      "outputs": [],
      "source": [
        "# Sort the jokes based on BM25 scores in descending order\n",
        "expanded_ranked_jokes = sorted(zip(all_jokes, bm25_scores), key=lambda x: x[1], reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVDunckA5PjE",
        "outputId": "7fda982a-c66d-4e18-afc1-540c138b4d39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Apparently the corona virus has been around since the 1800s But it used to spread by COVID wagons', 22.307948314746458), ('A single person be like: Corona vir-you + corona vir-me = corona virus.', 21.66872211038533), ('The National Center for Disease Control just downgraded the coronavirus. They say it’s a corona lite virus now.', 21.53273564512857), ('What’s the difference between Covid 19 and Romeo &amp; Juliet? ..One’s a Corona virus and the other’s a Verona crisis.', 21.28338691183202), ('I’ll have a Corona... hold the virus!', 21.219569152196243)]\n"
          ]
        }
      ],
      "source": [
        "print(expanded_ranked_jokes[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize my information extraction system, I created an interface using JupyterDash."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/dj/zfg5k_k10wj_lfz77pv5j4d00000gn/T/ipykernel_62322/2234723743.py:2: UserWarning: \n",
            "The dash_core_components package is deprecated. Please replace\n",
            "`import dash_core_components as dcc` with `from dash import dcc`\n",
            "  import dash_core_components as dcc\n",
            "/var/folders/dj/zfg5k_k10wj_lfz77pv5j4d00000gn/T/ipykernel_62322/2234723743.py:3: UserWarning: \n",
            "The dash_html_components package is deprecated. Please replace\n",
            "`import dash_html_components as html` with `from dash import html`\n",
            "  import dash_html_components as html\n"
          ]
        }
      ],
      "source": [
        "from jupyter_dash import JupyterDash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "from dash.dependencies import Input, Output, State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dash is running on http://127.0.0.1:8050/\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"650\"\n",
              "            src=\"http://127.0.0.1:8050/\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x15e8d9480>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mPreprocessingPipeline.preprocess_text\u001b[1;34m(\n",
            "    self=<__main__.PreprocessingPipeline object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n",
            "\u001b[1;32m---> 41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <__main__.PreprocessingPipeline object at 0x178f6b130>\u001b[0m\n",
            "\u001b[0;32m     42\u001b[0m     preprocessed_tokens \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
            "\n",
            "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mPreprocessingPipeline.tokenize\u001b[1;34m(\n",
            "    self=<__main__.PreprocessingPipeline object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n",
            "\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        text \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(\n",
            "    text=None,\n",
            "    language='english',\n",
            "    preserve_line=False\n",
            ")\u001b[0m\n",
            "\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n",
            "\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n",
            "\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "        preserve_line \u001b[1;34m= False\u001b[0m\u001b[1;34m\n",
            "        \u001b[0m[text] \u001b[1;34m= [None]\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mtext \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mlanguage \u001b[1;34m= 'english'\u001b[0m\n",
            "\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n",
            "\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n",
            "\u001b[0;32m    132\u001b[0m     ]\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text=None, language='english')\u001b[0m\n",
            "\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n",
            "\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n",
            "\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        tokenizer \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mtext \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n",
            "\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mrealign_boundaries \u001b[1;34m= True\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n",
            "\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n",
            "\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n",
            "\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n",
            "\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n",
            "\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mrealign_boundaries \u001b[1;34m= True\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(\n",
            "    .0=<generator object PunktSentenceTokenizer.span_tokenize>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n",
            "\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n",
            "\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n",
            "\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n",
            "\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n",
            "\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
            "        text \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n",
            "\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n",
            "\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n",
            "        slices \u001b[1;34m= <generator object PunktSentenceTokenizer._realign_boundaries at 0x15eb18740>\u001b[0m\n",
            "\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    slices=<generator object PunktSentenceTokenizer._slices_from_text>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n",
            "\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n",
            "\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n",
            "        slices \u001b[1;34m= <generator object PunktSentenceTokenizer._slices_from_text at 0x15eb18f90>\u001b[0m\n",
            "\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n",
            "\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(\n",
            "    iterator=<generator object PunktSentenceTokenizer._slices_from_text>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n",
            "\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
            "        iterator \u001b[1;34m= <generator object PunktSentenceTokenizer._slices_from_text at 0x15eb18f90>\u001b[0m\n",
            "\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n",
            "\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n",
            "\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself._lang_vars \u001b[1;34m= <nltk.tokenize.punkt.PunktLanguageVars object at 0x129a23970>\u001b[0m\n",
            "\u001b[0;32m   1396\u001b[0m \n",
            "\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n",
            "\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n",
            "\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
            "\n",
            "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object\n",
            "\n",
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[1;32mIn[7], line 41\u001b[0m, in \u001b[0;36mPreprocessingPipeline.preprocess_text\u001b[1;34m(\n",
            "    self=<__main__.PreprocessingPipeline object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n",
            "\u001b[1;32m---> 41\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <__main__.PreprocessingPipeline object at 0x178f6b130>\u001b[0m\n",
            "\u001b[0;32m     42\u001b[0m     preprocessed_tokens \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n",
            "\n",
            "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mPreprocessingPipeline.tokenize\u001b[1;34m(\n",
            "    self=<__main__.PreprocessingPipeline object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n",
            "\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        text \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(\n",
            "    text=None,\n",
            "    language='english',\n",
            "    preserve_line=False\n",
            ")\u001b[0m\n",
            "\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n",
            "\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n",
            "\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
            "        preserve_line \u001b[1;34m= False\u001b[0m\u001b[1;34m\n",
            "        \u001b[0m[text] \u001b[1;34m= [None]\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mtext \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mlanguage \u001b[1;34m= 'english'\u001b[0m\n",
            "\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n",
            "\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n",
            "\u001b[0;32m    132\u001b[0m     ]\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text=None, language='english')\u001b[0m\n",
            "\u001b[0;32m     97\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n",
            "\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n",
            "\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
            "        tokenizer \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mtext \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1278\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1279\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n",
            "\u001b[0;32m   1280\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mrealign_boundaries \u001b[1;34m= True\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n",
            "\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n",
            "\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n",
            "\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n",
            "\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n",
            "\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mrealign_boundaries \u001b[1;34m= True\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(\n",
            "    .0=<generator object PunktSentenceTokenizer.span_tokenize>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\n",
            "\u001b[0;32m   1333\u001b[0m     \u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, realign_boundaries: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[0;32m   1334\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n",
            "\u001b[0;32m   1335\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n",
            "\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n",
            "\u001b[0;32m   1338\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n",
            "\u001b[0;32m   1339\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n",
            "\u001b[0;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[1;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
            "        text \u001b[1;34m= None\u001b[0m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    realign_boundaries=True\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n",
            "\u001b[0;32m   1328\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n",
            "\u001b[1;32m-> 1329\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n",
            "        slices \u001b[1;34m= <generator object PunktSentenceTokenizer._realign_boundaries at 0x15eb1a0a0>\u001b[0m\n",
            "\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None,\n",
            "    slices=<generator object PunktSentenceTokenizer._slices_from_text>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1446\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1447\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n",
            "\u001b[0;32m   1448\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n",
            "\u001b[1;32m   (...)\u001b[0m\n",
            "\u001b[0;32m   1456\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n",
            "\u001b[0;32m   1457\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
            "\u001b[0;32m   1458\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;32m-> 1459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n",
            "        slices \u001b[1;34m= <generator object PunktSentenceTokenizer._slices_from_text at 0x15eb19ee0>\u001b[0m\n",
            "\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n",
            "\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(\n",
            "    iterator=<generator object PunktSentenceTokenizer._slices_from_text>\n",
            ")\u001b[0m\n",
            "\u001b[0;32m    319\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n",
            "\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[1;32m--> 321\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
            "        iterator \u001b[1;34m= <generator object PunktSentenceTokenizer._slices_from_text at 0x15eb19ee0>\u001b[0m\n",
            "\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mslice\u001b[39m]:\n",
            "\u001b[0;32m   1430\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[1;32m-> 1431\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_potential_end_contexts(text):\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\n",
            "\u001b[0;32m   1432\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n",
            "\u001b[0;32m   1433\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
            "\n",
            "File \u001b[1;32m~/Desktop/MASTER/second semester/info extraction/IR project/myenv/lib/python3.10/site-packages/nltk/tokenize/punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(\n",
            "    self=<nltk.tokenize.punkt.PunktSentenceTokenizer object>,\n",
            "    text=None\n",
            ")\u001b[0m\n",
            "\u001b[0;32m   1393\u001b[0m previous_slice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
            "\u001b[0;32m   1394\u001b[0m previous_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[1;32m-> 1395\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n",
            "        text \u001b[1;34m= None\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself \u001b[1;34m= <nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x129a23b50>\u001b[0m\u001b[1;34m\n",
            "        \u001b[0mself._lang_vars \u001b[1;34m= <nltk.tokenize.punkt.PunktLanguageVars object at 0x129a23970>\u001b[0m\n",
            "\u001b[0;32m   1396\u001b[0m \n",
            "\u001b[0;32m   1397\u001b[0m     \u001b[38;5;66;03m# Get the slice of the previous word\u001b[39;00m\n",
            "\u001b[0;32m   1398\u001b[0m     before_text \u001b[38;5;241m=\u001b[39m text[previous_slice\u001b[38;5;241m.\u001b[39mstop : match\u001b[38;5;241m.\u001b[39mstart()]\n",
            "\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_last_whitespace_index(before_text)\n",
            "\n",
            "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object\n",
            "\n"
          ]
        }
      ],
      "source": [
        "app = JupyterDash(__name__)\n",
        "\n",
        "external_stylesheets = [\n",
        "    \"https://cdnjs.cloudflare.com/ajax/libs/normalize/7.0.0/normalize.min.css\",\n",
        "    \"https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css\",\n",
        "    \"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700&display=swap\",\n",
        "    \"style.css\"\n",
        "]\n",
        "\n",
        "app = JupyterDash(__name__, external_stylesheets=external_stylesheets)\n",
        "\n",
        "app.layout = html.Div(\n",
        "    className=\"container\",\n",
        "    style={\n",
        "        \"display\": \"flex\",\n",
        "        \"justify-content\": \"center\",\n",
        "        \"align-items\": \"center\",\n",
        "        \"height\": \"100vh\",\n",
        "        \"background-image\": \"url('assets/harold.jpg')\",  # Specify the URL or path to your background image\n",
        "        \"background-size\": \"cover\",\n",
        "        \"background-position\": \"center\"\n",
        "    },\n",
        "    children=[\n",
        "        html.Div(\n",
        "            className=\"content\",\n",
        "            children=[\n",
        "                html.H1(\n",
        "                    \"The Dad Joke Generator\",\n",
        "                    style={\"font-family\": \"Comic Sans MS\", \"font-weight\": \"bold\", \"margin-bottom\": \"20px\"}  # Set the font to Comic Sans\n",
        "                ),\n",
        "                dcc.Input(\n",
        "                    id=\"query-input\",\n",
        "                    type=\"text\",\n",
        "                    placeholder=\"generate a dad joke containing this word\",\n",
        "                    style={\"width\": \"400px\", \"margin-bottom\": \"10px\"},\n",
        "                    className=\"input-field\"  # Add a custom CSS class\n",
        "                ),\n",
        "                html.Button(\"generate\", id=\"search-button\", n_clicks=0),\n",
        "                html.Div(\n",
        "                    id=\"result-output\",\n",
        "                    style={\n",
        "                        'border': '1px solid black',\n",
        "                        'padding': '10px',\n",
        "                        'margin-top': '20px',\n",
        "                        'background-color': 'white',\n",
        "                        \"font-family\": \"Comic Sans MS\"\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            ]\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n",
        "@app.callback(\n",
        "    Output('result-output', 'children'),\n",
        "    [Input('search-button', 'n_clicks')],\n",
        "    [State('query-input', 'value')]\n",
        ")\n",
        "def perform_search(n_clicks, query):\n",
        "    query_tokens = preprocessor.preprocess_text(query)\n",
        "\n",
        "    query_embeddings = [model.wv[token] for token in query_tokens]\n",
        "\n",
        "    similar_terms = []\n",
        "    for token in query_tokens:\n",
        "        similar_tokens = model.wv.most_similar(token, topn=3)  # Adjust the number of similar terms as desired\n",
        "        similar_terms.extend([sim_term[0] for sim_term in similar_tokens])\n",
        "\n",
        "    expanded_query = query_tokens + similar_terms\n",
        "\n",
        "    bm25 = BM25Okapi(preprocessed_jokes)\n",
        "    bm25_scores = bm25.get_scores(expanded_query)\n",
        "    expanded_ranked_jokes = sorted(zip(all_jokes, bm25_scores), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    results = []\n",
        "    try:\n",
        "        for rank, (joke, score) in enumerate(expanded_ranked_jokes[:5], 1):  # Print top 3 most relevant jokes\n",
        "            result = html.P(f\"Joke {rank}: {joke}\")\n",
        "            results.append(result)\n",
        "    except IndexError:\n",
        "        results.append(\"I'm out of inspiration. Someone find me a dad!\")\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(mode='inline')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cac635b2bbe20859b80c6b711019aad22d51ab596f6bdd91b0296498d6b3846b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
